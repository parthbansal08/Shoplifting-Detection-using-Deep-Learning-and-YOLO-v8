{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbd3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 not_shoplifting, 90.9ms\n",
      "Speed: 0.0ms preprocess, 90.9ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.3ms\n",
      "Speed: 3.0ms preprocess, 96.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 99.5ms\n",
      "Speed: 5.5ms preprocess, 99.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.0ms\n",
      "Speed: 0.0ms preprocess, 104.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 106.3ms\n",
      "Speed: 2.6ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 97.0ms\n",
      "Speed: 0.0ms preprocess, 97.0ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.9ms\n",
      "Speed: 0.0ms preprocess, 96.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 95.7ms\n",
      "Speed: 5.0ms preprocess, 95.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 93.0ms\n",
      "Speed: 5.6ms preprocess, 93.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 101.1ms\n",
      "Speed: 6.5ms preprocess, 101.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 93.7ms\n",
      "Speed: 2.6ms preprocess, 93.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 102.9ms\n",
      "Speed: 1.8ms preprocess, 102.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 99.0ms\n",
      "Speed: 6.1ms preprocess, 99.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 98.7ms\n",
      "Speed: 0.0ms preprocess, 98.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 101.5ms\n",
      "Speed: 0.0ms preprocess, 101.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 106.1ms\n",
      "Speed: 3.4ms preprocess, 106.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 99.4ms\n",
      "Speed: 4.8ms preprocess, 99.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 99.1ms\n",
      "Speed: 5.1ms preprocess, 99.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.7ms\n",
      "Speed: 4.6ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 106.4ms\n",
      "Speed: 0.0ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.0ms\n",
      "Speed: 5.6ms preprocess, 96.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 115.4ms\n",
      "Speed: 0.0ms preprocess, 115.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 123.2ms\n",
      "Speed: 5.0ms preprocess, 123.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 128.2ms\n",
      "Speed: 0.0ms preprocess, 128.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 114.6ms\n",
      "Speed: 4.4ms preprocess, 114.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 120.4ms\n",
      "Speed: 2.7ms preprocess, 120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 126.4ms\n",
      "Speed: 0.0ms preprocess, 126.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 123.6ms\n",
      "Speed: 4.0ms preprocess, 123.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 111.3ms\n",
      "Speed: 0.0ms preprocess, 111.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 110.8ms\n",
      "Speed: 6.1ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 112.7ms\n",
      "Speed: 5.2ms preprocess, 112.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 129.2ms\n",
      "Speed: 0.0ms preprocess, 129.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 114.7ms\n",
      "Speed: 0.0ms preprocess, 114.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 115.7ms\n",
      "Speed: 1.7ms preprocess, 115.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 132.9ms\n",
      "Speed: 5.7ms preprocess, 132.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 121.3ms\n",
      "Speed: 5.0ms preprocess, 121.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 106.8ms\n",
      "Speed: 8.1ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 119.4ms\n",
      "Speed: 2.6ms preprocess, 119.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 122.7ms\n",
      "Speed: 1.5ms preprocess, 122.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 108.4ms\n",
      "Speed: 1.0ms preprocess, 108.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.2ms\n",
      "Speed: 5.0ms preprocess, 104.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 103.1ms\n",
      "Speed: 2.6ms preprocess, 103.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 115.2ms\n",
      "Speed: 0.0ms preprocess, 115.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.4ms\n",
      "Speed: 8.1ms preprocess, 104.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 101.2ms\n",
      "Speed: 0.0ms preprocess, 101.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 111.9ms\n",
      "Speed: 1.5ms preprocess, 111.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 133.7ms\n",
      "Speed: 0.0ms preprocess, 133.7ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 108.8ms\n",
      "Speed: 0.0ms preprocess, 108.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 99.2ms\n",
      "Speed: 8.0ms preprocess, 99.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 116.4ms\n",
      "Speed: 0.0ms preprocess, 116.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 101.2ms\n",
      "Speed: 4.1ms preprocess, 101.2ms inference, 8.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 133.5ms\n",
      "Speed: 0.0ms preprocess, 133.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 114.9ms\n",
      "Speed: 2.6ms preprocess, 114.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 105.4ms\n",
      "Speed: 3.5ms preprocess, 105.4ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 99.4ms\n",
      "Speed: 0.0ms preprocess, 99.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 102.5ms\n",
      "Speed: 0.0ms preprocess, 102.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 141.8ms\n",
      "Speed: 10.7ms preprocess, 141.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 102.2ms\n",
      "Speed: 0.0ms preprocess, 102.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 107.5ms\n",
      "Speed: 3.8ms preprocess, 107.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 110.5ms\n",
      "Speed: 0.0ms preprocess, 110.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 88.4ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.0ms preprocess, 88.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.2ms\n",
      "Speed: 3.2ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.0ms\n",
      "Speed: 0.0ms preprocess, 96.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.8ms\n",
      "Speed: 1.7ms preprocess, 104.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 97.2ms\n",
      "Speed: 3.1ms preprocess, 97.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 94.3ms\n",
      "Speed: 0.0ms preprocess, 94.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 107.4ms\n",
      "Speed: 0.0ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 107.6ms\n",
      "Speed: 3.0ms preprocess, 107.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.3ms\n",
      "Speed: 0.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 101.3ms\n",
      "Speed: 6.6ms preprocess, 101.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 98.7ms\n",
      "Speed: 0.0ms preprocess, 98.7ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 128.8ms\n",
      "Speed: 0.0ms preprocess, 128.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 107.6ms\n",
      "Speed: 2.1ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 98.5ms\n",
      "Speed: 1.6ms preprocess, 98.5ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 93.6ms\n",
      "Speed: 9.1ms preprocess, 93.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 100.5ms\n",
      "Speed: 8.1ms preprocess, 100.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 107.8ms\n",
      "Speed: 2.0ms preprocess, 107.8ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 107.1ms\n",
      "Speed: 0.0ms preprocess, 107.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 85.1ms\n",
      "Speed: 0.0ms preprocess, 85.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 100.6ms\n",
      "Speed: 2.3ms preprocess, 100.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 144.3ms\n",
      "Speed: 5.2ms preprocess, 144.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 157.7ms\n",
      "Speed: 2.5ms preprocess, 157.7ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 139.2ms\n",
      "Speed: 1.1ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 113.1ms\n",
      "Speed: 4.9ms preprocess, 113.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.0ms\n",
      "Speed: 0.0ms preprocess, 96.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 105.5ms\n",
      "Speed: 0.0ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 112.3ms\n",
      "Speed: 6.5ms preprocess, 112.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 99.4ms\n",
      "Speed: 0.0ms preprocess, 99.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 99.6ms\n",
      "Speed: 7.4ms preprocess, 99.6ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 103.8ms\n",
      "Speed: 3.3ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 103.9ms\n",
      "Speed: 2.8ms preprocess, 103.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 97.2ms\n",
      "Speed: 0.0ms preprocess, 97.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 109.3ms\n",
      "Speed: 3.0ms preprocess, 109.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 101.2ms\n",
      "Speed: 2.6ms preprocess, 101.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 107.2ms\n",
      "Speed: 2.1ms preprocess, 107.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 106.8ms\n",
      "Speed: 7.7ms preprocess, 106.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 113.4ms\n",
      "Speed: 0.0ms preprocess, 113.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 110.3ms\n",
      "Speed: 5.3ms preprocess, 110.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 105.6ms\n",
      "Speed: 0.0ms preprocess, 105.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 113.8ms\n",
      "Speed: 1.2ms preprocess, 113.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 111.1ms\n",
      "Speed: 6.0ms preprocess, 111.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 106.5ms\n",
      "Speed: 5.0ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 102.9ms\n",
      "Speed: 3.9ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 105.1ms\n",
      "Speed: 1.6ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 2 shopliftings, 95.5ms\n",
      "Speed: 1.6ms preprocess, 95.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 2 shopliftings, 110.8ms\n",
      "Speed: 0.0ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 117.8ms\n",
      "Speed: 0.0ms preprocess, 117.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 101.4ms\n",
      "Speed: 4.2ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 104.9ms\n",
      "Speed: 0.0ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 108.4ms\n",
      "Speed: 2.4ms preprocess, 108.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 112.8ms\n",
      "Speed: 2.3ms preprocess, 112.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 108.2ms\n",
      "Speed: 6.4ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 101.8ms\n",
      "Speed: 5.1ms preprocess, 101.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 112.2ms\n",
      "Speed: 6.9ms preprocess, 112.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 133.6ms\n",
      "Speed: 4.3ms preprocess, 133.6ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 124.6ms\n",
      "Speed: 0.0ms preprocess, 124.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 138.0ms\n",
      "Speed: 0.0ms preprocess, 138.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 129.7ms\n",
      "Speed: 5.1ms preprocess, 129.7ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 115.4ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 115.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 112.5ms\n",
      "Speed: 5.0ms preprocess, 112.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 119.2ms\n",
      "Speed: 2.3ms preprocess, 119.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 112.5ms\n",
      "Speed: 0.0ms preprocess, 112.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 103.4ms\n",
      "Speed: 1.5ms preprocess, 103.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 102.9ms\n",
      "Speed: 0.0ms preprocess, 102.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 99.1ms\n",
      "Speed: 1.5ms preprocess, 99.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 98.1ms\n",
      "Speed: 2.5ms preprocess, 98.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 92.5ms\n",
      "Speed: 3.5ms preprocess, 92.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 85.0ms\n",
      "Speed: 3.2ms preprocess, 85.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 89.6ms\n",
      "Speed: 0.0ms preprocess, 89.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 94.3ms\n",
      "Speed: 0.0ms preprocess, 94.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 93.9ms\n",
      "Speed: 2.2ms preprocess, 93.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 87.8ms\n",
      "Speed: 1.9ms preprocess, 87.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 91.1ms\n",
      "Speed: 0.0ms preprocess, 91.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 85.5ms\n",
      "Speed: 2.7ms preprocess, 85.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 96.5ms\n",
      "Speed: 0.0ms preprocess, 96.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 87.5ms\n",
      "Speed: 8.5ms preprocess, 87.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 89.3ms\n",
      "Speed: 7.1ms preprocess, 89.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 89.0ms\n",
      "Speed: 2.0ms preprocess, 89.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 81.1ms\n",
      "Speed: 0.5ms preprocess, 81.1ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 79.4ms\n",
      "Speed: 8.6ms preprocess, 79.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 90.1ms\n",
      "Speed: 0.0ms preprocess, 90.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 78.1ms\n",
      "Speed: 2.5ms preprocess, 78.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 88.0ms\n",
      "Speed: 6.5ms preprocess, 88.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 91.2ms\n",
      "Speed: 3.5ms preprocess, 91.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 91.3ms\n",
      "Speed: 2.6ms preprocess, 91.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 105.0ms\n",
      "Speed: 9.0ms preprocess, 105.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 107.7ms\n",
      "Speed: 3.5ms preprocess, 107.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 97.4ms\n",
      "Speed: 2.8ms preprocess, 97.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 100.5ms\n",
      "Speed: 2.5ms preprocess, 100.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.5ms\n",
      "Speed: 6.6ms preprocess, 90.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 130.5ms\n",
      "Speed: 4.3ms preprocess, 130.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 101.1ms\n",
      "Speed: 2.1ms preprocess, 101.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 95.3ms\n",
      "Speed: 0.0ms preprocess, 95.3ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 114.8ms\n",
      "Speed: 7.0ms preprocess, 114.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 102.6ms\n",
      "Speed: 4.2ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.5ms\n",
      "Speed: 2.5ms preprocess, 96.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 106.1ms\n",
      "Speed: 3.1ms preprocess, 106.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 96.8ms\n",
      "Speed: 4.1ms preprocess, 96.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 96.3ms\n",
      "Speed: 3.9ms preprocess, 96.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 94.0ms\n",
      "Speed: 3.3ms preprocess, 94.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 98.5ms\n",
      "Speed: 3.0ms preprocess, 98.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 102.4ms\n",
      "Speed: 2.8ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 97.0ms\n",
      "Speed: 0.0ms preprocess, 97.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 85.3ms\n",
      "Speed: 8.1ms preprocess, 85.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.1ms\n",
      "Speed: 0.0ms preprocess, 90.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.4ms\n",
      "Speed: 0.0ms preprocess, 96.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 89.2ms\n",
      "Speed: 0.0ms preprocess, 89.2ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 94.4ms\n",
      "Speed: 0.0ms preprocess, 94.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 133.3ms\n",
      "Speed: 5.1ms preprocess, 133.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 95.1ms\n",
      "Speed: 0.0ms preprocess, 95.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.4ms\n",
      "Speed: 4.5ms preprocess, 90.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 104.6ms\n",
      "Speed: 0.0ms preprocess, 104.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 92.2ms\n",
      "Speed: 0.0ms preprocess, 92.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 88.5ms\n",
      "Speed: 4.1ms preprocess, 88.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 98.8ms\n",
      "Speed: 0.0ms preprocess, 98.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 97.3ms\n",
      "Speed: 0.0ms preprocess, 97.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 92.4ms\n",
      "Speed: 2.3ms preprocess, 92.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 85.0ms\n",
      "Speed: 4.1ms preprocess, 85.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 95.1ms\n",
      "Speed: 0.0ms preprocess, 95.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 88.1ms\n",
      "Speed: 0.0ms preprocess, 88.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.2ms\n",
      "Speed: 0.0ms preprocess, 90.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 106.4ms\n",
      "Speed: 2.0ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 97.3ms\n",
      "Speed: 3.7ms preprocess, 97.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 95.9ms\n",
      "Speed: 3.1ms preprocess, 95.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 95.9ms\n",
      "Speed: 6.6ms preprocess, 95.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 106.2ms\n",
      "Speed: 2.6ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 142.8ms\n",
      "Speed: 5.7ms preprocess, 142.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 94.5ms\n",
      "Speed: 2.6ms preprocess, 94.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 95.9ms\n",
      "Speed: 6.5ms preprocess, 95.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 100.7ms\n",
      "Speed: 0.0ms preprocess, 100.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 95.9ms\n",
      "Speed: 0.0ms preprocess, 95.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 91.0ms\n",
      "Speed: 2.8ms preprocess, 91.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 89.8ms\n",
      "Speed: 2.0ms preprocess, 89.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 81.6ms\n",
      "Speed: 0.5ms preprocess, 81.6ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 88.0ms\n",
      "Speed: 8.5ms preprocess, 88.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 87.0ms\n",
      "Speed: 0.0ms preprocess, 87.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 88.9ms\n",
      "Speed: 2.5ms preprocess, 88.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 89.4ms\n",
      "Speed: 0.0ms preprocess, 89.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 88.4ms\n",
      "Speed: 0.0ms preprocess, 88.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 85.3ms\n",
      "Speed: 2.4ms preprocess, 85.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 91.0ms\n",
      "Speed: 3.0ms preprocess, 91.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 86.0ms\n",
      "Speed: 2.3ms preprocess, 86.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 89.6ms\n",
      "Speed: 1.9ms preprocess, 89.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 96.0ms\n",
      "Speed: 0.0ms preprocess, 96.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 98.3ms\n",
      "Speed: 5.6ms preprocess, 98.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 97.9ms\n",
      "Speed: 6.1ms preprocess, 97.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.7ms\n",
      "Speed: 0.0ms preprocess, 90.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 88.0ms\n",
      "Speed: 4.0ms preprocess, 88.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.6ms\n",
      "Speed: 0.0ms preprocess, 90.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.2ms\n",
      "Speed: 0.0ms preprocess, 90.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 94.8ms\n",
      "Speed: 2.9ms preprocess, 94.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 88.0ms\n",
      "Speed: 1.1ms preprocess, 88.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 94.4ms\n",
      "Speed: 0.0ms preprocess, 94.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 95.2ms\n",
      "Speed: 3.5ms preprocess, 95.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 96.6ms\n",
      "Speed: 0.0ms preprocess, 96.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 88.5ms\n",
      "Speed: 0.0ms preprocess, 88.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 87.7ms\n",
      "Speed: 2.5ms preprocess, 87.7ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 90.7ms\n",
      "Speed: 5.1ms preprocess, 90.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 88.5ms\n",
      "Speed: 0.0ms preprocess, 88.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 89.6ms\n",
      "Speed: 1.9ms preprocess, 89.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 95.8ms\n",
      "Speed: 0.0ms preprocess, 95.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 93.3ms\n",
      "Speed: 2.5ms preprocess, 93.3ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 90.2ms\n",
      "Speed: 1.6ms preprocess, 90.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 82.9ms\n",
      "Speed: 1.5ms preprocess, 82.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 shoplifting, 93.5ms\n",
      "Speed: 2.5ms preprocess, 93.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 92.2ms\n",
      "Speed: 8.1ms preprocess, 92.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 87.0ms\n",
      "Speed: 3.5ms preprocess, 87.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 97.5ms\n",
      "Speed: 0.0ms preprocess, 97.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 95.8ms\n",
      "Speed: 6.1ms preprocess, 95.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 90.2ms\n",
      "Speed: 0.0ms preprocess, 90.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 98.0ms\n",
      "Speed: 6.1ms preprocess, 98.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 117.6ms\n",
      "Speed: 2.5ms preprocess, 117.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 105.9ms\n",
      "Speed: 1.6ms preprocess, 105.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 102.3ms\n",
      "Speed: 0.0ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 2 shopliftings, 105.2ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 6.5ms preprocess, 105.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 101.4ms\n",
      "Speed: 5.1ms preprocess, 101.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 113.6ms\n",
      "Speed: 0.0ms preprocess, 113.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 115.1ms\n",
      "Speed: 3.5ms preprocess, 115.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 105.2ms\n",
      "Speed: 4.0ms preprocess, 105.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 98.0ms\n",
      "Speed: 8.5ms preprocess, 98.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 85.8ms\n",
      "Speed: 4.6ms preprocess, 85.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 79.6ms\n",
      "Speed: 1.5ms preprocess, 79.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 99.7ms\n",
      "Speed: 0.0ms preprocess, 99.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 91.1ms\n",
      "Speed: 0.0ms preprocess, 91.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 80.0ms\n",
      "Speed: 1.5ms preprocess, 80.0ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 85.5ms\n",
      "Speed: 0.0ms preprocess, 85.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 93.1ms\n",
      "Speed: 3.0ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 86.5ms\n",
      "Speed: 5.0ms preprocess, 86.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 93.6ms\n",
      "Speed: 0.0ms preprocess, 93.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 86.8ms\n",
      "Speed: 1.5ms preprocess, 86.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 88.7ms\n",
      "Speed: 0.0ms preprocess, 88.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 96.0ms\n",
      "Speed: 0.0ms preprocess, 96.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 91.3ms\n",
      "Speed: 0.0ms preprocess, 91.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 shopliftings, 78.4ms\n",
      "Speed: 0.0ms preprocess, 78.4ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 shoplifting, 88.0ms\n",
      "Speed: 0.0ms preprocess, 88.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 shoplifting, 105.5ms\n",
      "Speed: 0.0ms preprocess, 105.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 shopliftings, 85.5ms\n",
      "Speed: 3.6ms preprocess, 85.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 not_shopliftings, 1 shoplifting, 96.2ms\n",
      "Speed: 0.0ms preprocess, 96.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 95.7ms\n",
      "Speed: 2.4ms preprocess, 95.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 90.9ms\n",
      "Speed: 3.1ms preprocess, 90.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 85.8ms\n",
      "Speed: 4.0ms preprocess, 85.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 82.7ms\n",
      "Speed: 5.0ms preprocess, 82.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 93.1ms\n",
      "Speed: 0.0ms preprocess, 93.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 not_shoplifting, 1 shoplifting, 113.9ms\n",
      "Speed: 0.0ms preprocess, 113.9ms inference, 9.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "#import parameters\n",
    "from config.parameters import WIDTH,start_status,shoplifting_status,not_shoplifting_status\n",
    "from config.parameters import cls0_rect_color,cls1_rect_color,conf_color,status_color\n",
    "from config.parameters import quit_key,frame_name\n",
    "\n",
    "input_path=\"res\\inout1.mp4\"\n",
    "output_path=\"\"\n",
    "\n",
    "#model\n",
    "mymodel=YOLO(\"configs\\shoplifting_wights.pt\")\n",
    "\n",
    "# input fils\n",
    "cap = cv2.VideoCapture(input_path)  \n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = imutils.resize(frame, width=WIDTH)\n",
    "    \n",
    "    result=mymodel.predict(frame)\n",
    "    cc_data=np.array(result[0].boxes.data)\n",
    "\n",
    "    if len(cc_data) != 0:\n",
    "                xywh=np.array(result[0].boxes.xywh).astype(\"int32\")\n",
    "                xyxy=np.array(result[0].boxes.xyxy).astype(\"int32\")\n",
    "                \n",
    "                for (x1, y1, _, _), (_, _, w, h), (_,_,_,_,conf,clas) in zip(xyxy, xywh,cc_data):\n",
    "                            person = frame[y1:y1+h,x1:x1+w]\n",
    "                            status=start_status\n",
    "                            if clas==1:\n",
    "                                    cv2.rectangle(frame,(x1,y1),(x1+w,y1+h),cls1_rect_color,2)\n",
    "                                    half_w=w/2\n",
    "                                    half_h=h/2\n",
    "                                    x=int(half_w+x1)\n",
    "                                    cv2.circle(frame, (x, y1), 6, (0, 0, 255), 8)\n",
    "\n",
    "                                    text = \"{}%\".format(np.round(conf*100,2))\n",
    "                                    cv2.putText(frame, text, (x1+10,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,conf_color, 2)\n",
    "                                    status=shoplifting_status\n",
    "                                    \n",
    "                            elif clas==0 and conf>0.8:\n",
    "                                    cv2.rectangle(frame,(x1,y1),(x1+w,y1+h),cls0_rect_color,1)\n",
    "                                    text = \"{}%\".format(np.round(conf*100,2))\n",
    "                                    cv2.putText(frame, text, (x1+10,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,conf_color, 2)\n",
    "                                    status=not_shoplifting_status\n",
    "                cv2.putText(frame, status, (10,20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)\n",
    "\n",
    "    cv2.imshow(frame_name, frame)\n",
    "    \n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(quit_key):\n",
    "        break\n",
    "\n",
    "\n",
    "    if output_path != \"\" and writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(output_path, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    \n",
    "    if writer is not None:\n",
    "        print(\"[INFO] writing stream to output\")\n",
    "        writer.write(frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ac5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd143117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
